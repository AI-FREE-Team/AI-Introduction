{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJyIFimFTiwP"
      },
      "source": [
        "![人工智慧 - 自由團隊](https://raw.githubusercontent.com/chenkenanalytic/img/master/af/aifreeteam.png)\n",
        "\n",
        "\n",
        "<center>Welcome to the course《Python: from business analytics to Artificial Intelligence》by AI . FREE Team.</center>\n",
        "<br>\n",
        "<center>歡迎大家來到 AI . FREE Team 《Python 從商業分析到人工智慧》的 AI 基礎教學 - Lesson 03 Neural Network (3-03)。 </center>\n",
        "<br>\n",
        "\n",
        "<center>(Author: Chen Ken；Date of published: 2021//；AI . FREE Team Website: https://aifreeblog.herokuapp.com/)</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG2kAgokoYWF"
      },
      "source": [
        "# 類神經網路\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN6w7-92o7uY"
      },
      "source": [
        "在經過上一篇羅吉斯回歸的手刻演算法後，本篇教學，我們將帶大家走過基礎類神經網路的手刻模型；讀者們可以將羅吉斯回歸模型視作每一個類神經元，而神經網路即是將很多個神經元集結在做運算。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmE19QDujVeU"
      },
      "source": [
        "## STEP 0. 使用矩陣運算套件 (numpy)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM0UCPzWjb_s"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdUpf2XjqfIe"
      },
      "source": [
        "## STEP 1. Sigmoid 函數\n",
        "\n",
        "先 code 出 sigmoid 的機率函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIwkpsFJoA4G"
      },
      "source": [
        "def sigmoid(x):\n",
        "  p = 1 / (1 + np.exp(-x))\n",
        "  return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFH81c1VStyy"
      },
      "source": [
        "## STEP 2. 建構神經網絡大小\n",
        "\n",
        "定義「兩層」神經網路的神經元多寡。 (輸入層 & 隱藏層)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMgSpCy_Ss8b"
      },
      "source": [
        "def neurons(first_nums, second_nums):\n",
        "  # 神經元數量\n",
        "  in_num = first_nums\n",
        "  # 一層\n",
        "  hid_num = second_nums\n",
        "  return (in_num, hid_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfvquALnrJiu"
      },
      "source": [
        "## STEP 3. 初始化權重\n",
        "\n",
        "針對類神經網路的權重進行初始化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAtEidCHrcSw"
      },
      "source": [
        "def initial_weight(in_num, hid_num):\n",
        "\n",
        "  a1 = np.zeros(shape=(in_num, hid_num), dtype=\"float32\")\n",
        "  b1 = np.zeros(shape=(1, hid_num), dtype=\"float32\")\n",
        "  a2 = np.zeros(shape=(hid_num, 1), dtype=\"float32\")\n",
        "  b2 = np.zeros(shape=(1, 1), dtype=\"float32\")\n",
        "\n",
        "  parameters = {\"a1\": a1,\n",
        "                \"b1\": b1,\n",
        "                \"a2\": a2,\n",
        "                \"b2\": b2}\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w8JyChrse7C"
      },
      "source": [
        "## STEP 4. 向前傳播 Forward Probagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdH9gFd6seJF"
      },
      "source": [
        "def forward(parameters, x):\n",
        "\n",
        "  a1 = parameters[\"a1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  a2 = parameters[\"a2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "  # 進行向前傳播演算\n",
        "  z1 = np.dot(x, a1) + b1\n",
        "  y1 = sigmoid(z1)\n",
        "  z2 = np.dot(y1, a2) + b2\n",
        "  y2 = sigmoid(z2)\n",
        "\n",
        "  # 暫存資訊，作為梯度下降資訊\n",
        "  temp = {\"z1\": z1,\n",
        "          \"y1\": y1,\n",
        "          \"z2\": z2,\n",
        "          \"y2\": y2}\n",
        "\n",
        "  return y2, temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55lyPDvRxEfU"
      },
      "source": [
        "## STEP 5. 計算損失函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOS__1opxMMz"
      },
      "source": [
        "def loss_function(y2, Y, parameters):\n",
        "\n",
        "  # 找出資料筆數\n",
        "  m = Y.shape[1]\n",
        "\n",
        "  # 計算交叉商 cross-entropy loss\n",
        "  logprobs = np.multiply(np.log(y2), Y) + np.multiply(np.log(1 - y2), 1 - Y)\n",
        "  loss = -1 / m * np.sum(logprobs)\n",
        "  loss = np.squeeze(loss)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca8X60n8yyg8"
      },
      "source": [
        "## STEP 6. 向後傳播 Backward Probagation\n",
        "\n",
        "計算微分導數 (梯度)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfiE8zP3y5Oh"
      },
      "source": [
        "def backward(parameters, temp, x, Y):\n",
        "\n",
        "  # 找出資料筆數\n",
        "  m = x.shape[1]\n",
        "\n",
        "  a1 = parameters[\"a1\"]\n",
        "  a2 = parameters['a2']\n",
        "  y1 = temp[\"y1\"]\n",
        "  y2 = temp[\"y2\"]\n",
        "\n",
        "  # 計算第二層神經層權重梯度\n",
        "  dz2 = y2 - Y\n",
        "  da2 = 1 / m * np.dot(y1.T,dz2)\n",
        "  db2 = 1 / m * np.sum(dz2, axis=1, keepdims = True)\n",
        "\n",
        "  # 計算第一層神經層權重梯度\n",
        "  dz1 = np.dot(dz2, a2.T) * (1 - np.power(y1, 2))\n",
        "  da1 = 1 / m * np.dot(x.T, dz1)\n",
        "  db1 = 1 / m * np.sum(dz1, axis=1, keepdims = True)\n",
        "\n",
        "  diffs = {\"da1\": da1,\n",
        "           \"db1\": db1,\n",
        "           \"da2\": da2,\n",
        "           \"db2\": db2}\n",
        "\n",
        "  return diffs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "773H71-Jkqms"
      },
      "source": [
        "## STEP 7. 透過微分導數 (diffs) 更新模型權重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6OInPYH1eik"
      },
      "source": [
        "def updates(parameters, diffs, learning_rate = 1.0):\n",
        "\n",
        "  a1 = parameters[\"a1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  a2 = parameters[\"a2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "  da1 = diffs[\"da1\"]\n",
        "  db1 = diffs[\"db1\"]\n",
        "  da2 = diffs[\"da2\"]\n",
        "  db2 = diffs[\"db2\"]\n",
        "\n",
        "  # 根據梯度與學習率，更新權重\n",
        "  a1 = a1 - learning_rate * da1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "  a2 = a2 - learning_rate * da2\n",
        "  b2 = b2 - learning_rate * db2\n",
        "\n",
        "  parameters = {\"a1\": a1,\n",
        "                \"b1\": b1,\n",
        "                \"a2\": a2,\n",
        "                \"b2\": b2}\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIWx_-GdrinQ"
      },
      "source": [
        "## STEP 8. 合併所有自定義函數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb96-sso2c-Z"
      },
      "source": [
        "# GRADED FUNCTION: nn_model\n",
        "\n",
        "def neural_net(x, Y, second_nums, loops = 100):\n",
        "\n",
        "  np.random.seed(7)\n",
        "\n",
        "  # 找出第一層的特徵值\n",
        "  first_nums = x.shape[1]\n",
        "\n",
        "  # 定義每層神經元數量 (輸入層 & 隱藏層)\n",
        "  in_num, hid_num = neurons(first_nums, second_nums)\n",
        "\n",
        "  # 初始化神經元權重\n",
        "  parameters = initial_weight(in_num, hid_num)\n",
        "\n",
        "  a1 = parameters[\"a1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  a2 = parameters[\"a2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "  for i in range(0, loops):\n",
        "\n",
        "    # 向前傳播\n",
        "    y2, temp = forward(parameters, x)\n",
        "\n",
        "    # 計算損失函數\n",
        "    loss = loss_function(y2, Y, parameters)\n",
        "\n",
        "    # 計算梯度\n",
        "    diffs = backward(parameters, temp, x, Y)\n",
        "\n",
        "    # 更新模型權重\n",
        "    parameters = updates(parameters, diffs, learning_rate = 1.0)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "      print (\"第 \", i, \" 個迴圈; 目前模型運算的成本為: \", loss)\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Em8_FdRyaM6"
      },
      "source": [
        "## STEP 8. 運行與檢視"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmaiY9frygL4"
      },
      "source": [
        "### (1) 建立資料\n",
        "\n",
        "以 Lesson 1， and, or, xor 的資料集為例做學習判斷"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7rNBGCe4mG2"
      },
      "source": [
        "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "\n",
        "y_and = np.array([[0],[0],[0],[1]])\n",
        "y_or = np.array([[0],[1],[1],[1]])\n",
        "y_xor = np.array([[1],[0],[0],[1]])\n",
        "\n",
        "print(x.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxiCjdTIy0IZ"
      },
      "source": [
        "### (2) 開始訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8puUEjtV6hFl"
      },
      "source": [
        "print(\"【 and 資料學習】：\")\n",
        "parameters_and = neural_net(x, y_and, second_nums = 3)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"【 or 資料學習】：\")\n",
        "parameters_or = neural_net(x, y_or, second_nums = 3)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"【 xor 資料學習】：\")\n",
        "parameters_xor = neural_net(x, y_xor, second_nums = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRXYG82o9igu"
      },
      "source": [
        "### (3) 預測模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GQ-pZAG82YG"
      },
      "source": [
        "def predict(parameters, x):\n",
        "\n",
        "  y2, temp = forward(parameters, x)\n",
        "  predictions = np.round_(y2)\n",
        "\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPr1V21DJ87M"
      },
      "source": [
        "### (4) 查看模型預測結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCKagVHpy5Nb"
      },
      "source": [
        "prediction_and = predict(parameters_and ,x)\n",
        "print(\"y_and 類神經模型模型判斷正確率為: {} %\".format(100-np.mean(np.abs(prediction_and-y_and))*100))\n",
        "\n",
        "prediction_or = predict(parameters_or ,x)\n",
        "print(\"y_or 類神經模型模型判斷正確率為: {} %\".format(100-np.mean(np.abs(prediction_or-y_or))*100))\n",
        "\n",
        "prediction_xor = predict(parameters_xor ,x)\n",
        "print(\"y_xor 類神經模型模型判斷正確率為: {} %\".format(100-np.mean(np.abs(prediction_xor-y_xor))*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5fSUAV_NlGg"
      },
      "source": [
        "#結論與發現"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn07j2Xo0t08"
      },
      "source": [
        "透過兩層類神經模型的手刻程式碼，可以觀察到羅吉斯回歸就好比單一個神經元，\n",
        "\n",
        "多個羅吉斯回歸模型的堆疊，建置出一個類神經模型，同採用梯度下降(微分)來進行模型參數的更新，\n",
        "\n",
        "從實驗結果，我們可以發現類神經模型在邏輯運算(and, or, xor)皆能夠達到 100% 判斷正確率！\n",
        "\n",
        "成功證明類神經模型，解決複雜問題的有效性！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV0JVi5gM0wr"
      },
      "source": [
        "但...如果根據每次的任務，都需要重新撰寫程式碼，真的太過耗時，\n",
        "\n",
        "因此科學家們推出了實用的深度學習框架，供 AI 工程師得以快速開發類神經模型！\n",
        "\n",
        "下一篇教學，將引導讀者學習 AI 模型開源框架：Keras。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 更深入的教學與專案實作\n",
        "\n",
        "如果你/妳對於深入開發 AI 技術的專案有興趣，想嘗試更多、更有趣、更扎實的實務AI技術專案，\n",
        "\n",
        "歡迎參考我們從0到1的AI技術課程：[《學習 AI 一把抓：點亮人工智慧技能樹》](https://hahow.in/cr/slashie-ai-free-team)！"
      ],
      "metadata": {
        "id": "6X4jsuSKEN7E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9KkN2cXhgCB"
      },
      "source": [
        "# 課程文件\n",
        "\n",
        "### AI Foundations 課程清單\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 00 Preface 課程前言</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 01 MP model</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 02 Logistic Regression</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 03 Neural Network</a> (We are here now! --本篇課程--)\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 04 Deep Neural Network</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 05 Convolution Neural Network</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 06 Handwriting Traditional Chinese Characters Recognition</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 07 Sentiment Analysis</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4fDiivP5ajm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}