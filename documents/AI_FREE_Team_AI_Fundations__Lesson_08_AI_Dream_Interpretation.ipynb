{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJyIFimFTiwP"
      },
      "source": [
        "![人工智慧 - 自由團隊](https://raw.githubusercontent.com/chenkenanalytic/img/master/af/aifreeteam.png)\n",
        "\n",
        "\n",
        "<center>Welcome to the course《Python: from business analytics to Artificial Intelligence》by AI . FREE Team.</center>\n",
        "<br>\n",
        "<center>歡迎大家來到 AI . FREE Team 《Python 從商業分析到人工智慧》的 AI 基礎教學 - Lesson 08 AI Dream Interpretation (3-08)。 </center>\n",
        "<br>\n",
        "\n",
        "<center>(Author: Chen Ken；Date of published: 2021//；AI . FREE Team Website: https://aifreeblog.herokuapp.com/)</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG2kAgokoYWF"
      },
      "source": [
        "# 解夢文本生成 - Dream Interpretation Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN6w7-92o7uY"
      },
      "source": [
        "莊周夢蝶，哲學家討論著現實與夢境的虛實真假，究竟夢境中的情節是現實，還是只是反映生活的片段記憶？\n",
        "\n",
        "又或是對於未來的預測、生物本能的第六感？因此從古代人們透過《易經》，卜卦解析夢境與現實的關聯，\n",
        "\n",
        "進而預測未來事件的吉凶！解夢是機率統計？還是真的有其不可思議的力量？\n",
        "\n",
        "讓我們透過 AI 來學習周易解夢的邏輯，試看看 AI 是否也能有預測未來的能力吧！\n",
        "\n",
        "在上一篇教學，我們認識了 NLP 領域中的 NLU(自然語言理解)，本實作教學將帶讀者認識 NLG (自然語言生成)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmE19QDujVeU"
      },
      "source": [
        "## STEP 0. 下載資料集並解壓縮\n",
        "\n",
        "※ 資料集爬蟲自網路解夢相關網站，並已初步清理訓練文本。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM0UCPzWjb_s"
      },
      "source": [
        "### 資料上傳時間，依電腦網速而定 (共三個壓縮檔)\n",
        "!git clone https://github.com/AI-FREE-Team/AI-Introduction.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8j0c-vtr-C7"
      },
      "source": [
        "### 解壓縮資料集，路徑請依據實際情況調整\n",
        "!unzip \"/content/AI-Introduction/clean_all.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdUpf2XjqfIe"
      },
      "source": [
        "## STEP 1. 讀取資料集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIwkpsFJoA4G"
      },
      "source": [
        "### 讀取爬蟲解夢文字檔\n",
        "f = open('/content/clean_all.txt')\n",
        "text = str()\n",
        "for line in f:\n",
        "    text += line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GifyUBvc-Tc"
      },
      "source": [
        "### 檢視資料集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_8wLDxWt_zc"
      },
      "source": [
        "text[6023:6200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDT9j6I_dKWU"
      },
      "source": [
        "# 檢視文字檔內容\n",
        "n = len(text)\n",
        "w = len(set(text))\n",
        "print(f\"解夢資料共有 {n} 中文字\")\n",
        "print(f\"其中有 {w} 個獨一無二的字\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4IOspeZZnaf"
      },
      "source": [
        "## STEP 2. 文字資料集清理\n",
        "\n",
        "於本解夢實作，原始資料在爬蟲階段已經完成清理，以下程式碼取自 lesson 07 資料及清理方式，提供需要的讀者參酌。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwqaWJ6h7bUM"
      },
      "source": [
        "# # 導入相關文字清理套件\n",
        "\n",
        "# ### beautifulsoup 為常見的爬蟲使用套件，擅長解析網頁原始碼 (ref: https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# ### re 套件為正規表達式套件 (ref: https://docs.python.org/3/library/re.html)\n",
        "# import re\n",
        "\n",
        "# ### nltk 為自然語言處理常用套件 (ref: https://www.nltk.org/)\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords # 導入停用詞模組\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ucYoskLhwzI"
      },
      "source": [
        "# # 自定義文本清理的函數\n",
        "# def Preprocessing(text):\n",
        "\n",
        "#   # 將全部字詞轉為小寫\n",
        "#   text = text.lower()\n",
        "\n",
        "#   # 移除網頁編碼相關內容，例如：html原始碼\n",
        "#   text = BeautifulSoup(text).get_text()\n",
        "\n",
        "#   # 移除特殊字元\n",
        "#   text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
        "\n",
        "#   # 使用 nltk 套件，將文句切割成字詞列表\n",
        "#   wordList = nltk.word_tokenize(text)\n",
        "\n",
        "#   # # 使用字詞列表，刪除停用詞 -- 不使用\n",
        "#   # filtered = [w for w in wordList if w not in stopwords.words('english')]\n",
        "\n",
        "#   return \" \".join(wordList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h10a_NmNaLnR"
      },
      "source": [
        "# # 使用 for loop 清理文本資料 (執行約數分鐘)\n",
        "# for i in range(len(text))):\n",
        "#   text[i] = Preprocessing(text[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dQq4oq2AWWS"
      },
      "source": [
        "# # 快速檢視資料集\n",
        "# text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFH81c1VStyy"
      },
      "source": [
        "## STEP 3. 建立字詞索引 (Word Indexing)\n",
        "\n",
        "在進行自然語言訓練任務時，考量到神經網路的運算仍是以數字進行計算，因此我們必須建立數字索引對應到字詞，方能使神經網路進行訓練。\n",
        "\n",
        "以中文字建立數值索引為例，如：｛ 一： 0 , 丁： 1 , ... ... , 龜： 3978 , ... ｝"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf5DtSoU_u9q"
      },
      "source": [
        "# 使用 keras 內建套件，建立字詞索引 (word indexing)\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# 先建立空的索引\n",
        "tokenizer = Tokenizer(num_words = w, char_level = True, filters = '')\n",
        "\n",
        "# 讀取文本，找出所有單一字詞建立索引\n",
        "tokenizer.fit_on_texts(text)\n",
        "\n",
        "# 將原始文本轉換成索引形式 (儲存在 sequences)\n",
        "sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# 將索引對應的字詞找出，計算共有幾個獨立字詞\n",
        "word_index = tokenizer.word_index\n",
        "print('共有 ',  len(word_index), ' 獨立字詞')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8wiUHKJHh3U"
      },
      "source": [
        "# 檢視字詞索引\n",
        "word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xRVvNmbAcPn"
      },
      "source": [
        "## STEP 4. 訓練資料集前處理\n",
        "\n",
        "在文本生成的模型中，會將文本接續下一個字作為預測目標，簡單來說，第一個字預測目標為第二個字，第二字預測目標維第三個字...以此列舉，因此我們直接定義每一筆訓練數據的長度，label data 則為文本位移一個位置的數據。\n",
        "\n",
        "```\n",
        "eg.\n",
        "train_X = text[0:128]\n",
        "train_label = text[1:129]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sXPWoZ9GV_o"
      },
      "source": [
        "# 定義訓練輸入資料\n",
        "SEQ_LENGTH = 128  # 每一段訓練文字長度\n",
        "BATCH_SIZE = 128 # 每次訓練資料筆數"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h9BWnULG-Lh"
      },
      "source": [
        "# 我們目前的文本為 sequences (list)\n",
        "# 需轉換為 tensorflow 讀取資料的格式\n",
        "characters = tf.data.Dataset.from_tensor_slices(sequences)\n",
        "\n",
        "# 將以數值索引的解夢內容\n",
        "# 分成多個長度為 SEQ_LENGTH 的文句\n",
        "# 並將文句長度不滿 SEQ_LENGTH 的句子刪去\n",
        "data = characters.batch(SEQ_LENGTH + 1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8OHMVZLhhse"
      },
      "source": [
        "# 解夢訓練文本 - 成對的訓練資料組數\n",
        "steps_per_epoch = len(sequences) // SEQ_LENGTH\n",
        "\n",
        "# 透過此函式將每一段訓練資料\n",
        "# 拆成兩個部分，(1) 少了最後一個字 (2) 少了第一個字\n",
        "# 以利訓練資料做訓練 (資料集2為Label用途 → 對應每一個資料集1的接續文字)\n",
        "def build_seq_pairs(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# 將每個解夢文本索引資料套用上列公式\n",
        "# 作為 輸入／輸出 的訓練資料 (input / target)\n",
        "# 再透過 shuffle 將數據隨機排序\n",
        "# 再透過 BATCH_SIZE（128） 去定義完整的訓練資料集\n",
        "training_data = data.map(build_seq_pairs).shuffle(steps_per_epoch).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knlYmn9chYRw"
      },
      "source": [
        "## STEP 5. 建置 NLP 類神經模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiT_rYDoB7dp"
      },
      "source": [
        "# 定義超參數\n",
        "EMBEDDING_DIM = 512\n",
        "LSTM_UNITS = 1024\n",
        "\n",
        "# 使用 keras 定義模型\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# 使用 word embedding 技術\n",
        "model.add(tf.keras.layers.Embedding(input_dim=w, output_dim=EMBEDDING_DIM,batch_input_shape=[BATCH_SIZE, None]))\n",
        "\n",
        "# 使用 LSTM 模型\n",
        "model.add(tf.keras.layers.LSTM(units=LSTM_UNITS, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "# 使用 dense 類神經網路\n",
        "model.add(tf.keras.layers.Dense(w))\n",
        "\n",
        "#查看模型\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUSUVzqwB9nZ"
      },
      "source": [
        "## STEP 6. 模型訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVf2HXixiOP9"
      },
      "source": [
        "# 定義超參數 (學習率)\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# 定義模型預測結果跟正確結果之間的差異 (LOSS)\n",
        "# 因為全連接層沒使用 activation func\n",
        "# from_logits= True\n",
        "def loss(y_true, y_pred):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "\n",
        "# 使用 Adam 優化器\n",
        "# 使用上方定義的損失函數\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oKblVsi6f4G"
      },
      "source": [
        "# 定義訓練次數\n",
        "EPOCHS = 25\n",
        "\n",
        "# 開始訓練\n",
        "history = model.fit(\n",
        "    training_data, # 前面建置的資料集\n",
        "    epochs=EPOCHS #訓練次數\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofCVLuH79_yj"
      },
      "source": [
        "## STEP 7. 訓練模型儲存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa4ZQ0pI-G3V"
      },
      "source": [
        "# 儲存訓練完成的模型\n",
        "model.save('dream_interpretation_model_01.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93sGsZlXiGX_"
      },
      "source": [
        "## STEP 8. 解夢模型設定\n",
        "\n",
        "初步設定解夢模型架構，並讀取訓練模型權重。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6aquHAqnLW5"
      },
      "source": [
        "# 跟訓練時一樣的超參數，只差在 BATCH_SIZE 為 1\n",
        "EMBEDDING_DIM = 512\n",
        "LSTM_UNITS = 1024\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# 定義生成的模型\n",
        "infer_model = tf.keras.Sequential()\n",
        "\n",
        "# 使用 word embedding 技術\n",
        "infer_model.add(tf.keras.layers.Embedding(input_dim=w, output_dim=EMBEDDING_DIM, batch_input_shape=[BATCH_SIZE, None]))\n",
        "\n",
        "# 使用 LSTM 模型\n",
        "infer_model.add(tf.keras.layers.LSTM(units=LSTM_UNITS, return_sequences=True, stateful=True))\n",
        "\n",
        "# 使用 dense 類神經網路\n",
        "infer_model.add(tf.keras.layers.Dense(w))\n",
        "\n",
        "#查看模型\n",
        "infer_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoY1rzeqCk5p"
      },
      "source": [
        "# 載入已儲存模型之權重\n",
        "infer_model.load_weights('dream_interpretation_model_01.h5')\n",
        "infer_model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_zZYa_9GIHu"
      },
      "source": [
        "## STEP 9. 開始解夢\n",
        "\n",
        "使用 infer_model 進行解夢！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uMnGb8p3kEI"
      },
      "source": [
        "# 您的夢境\n",
        "text_generated = '夢見老虎'\n",
        "\n",
        "# 生成字數 → 建議不要太長或太短，20-40字內效果較佳\n",
        "nums_text = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWSwuCublgKF"
      },
      "source": [
        "# 使用 for loop 建立文本生成器\n",
        "for i in range(nums_text):\n",
        "\n",
        "  # 將夢境文本轉換成索引\n",
        "  dream = tokenizer.texts_to_sequences([text_generated])[0]\n",
        "\n",
        "  # 增加 batch 維度丟入模型取得預測結果後，再降維，拿掉 batch 維度\n",
        "  input = tf.expand_dims(dream, axis=0)\n",
        "  predictions = infer_model(input)\n",
        "  predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "  # 取得這個時間點模型生成的中文字\n",
        "  predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  partial_texts = [tokenizer.index_word[predicted_id]]\n",
        "  text_generated += partial_texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkKk8ei4BMt1"
      },
      "source": [
        "# 成功生成 解夢文字檔 → text_generated\n",
        "# 透過擷取解夢內容作呈現\n",
        "# print(text_generated)\n",
        "\n",
        "# 使用此程式碼，讓解夢文字更有結構化\n",
        "print(text_generated.split('\\n')[0].split('。')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5fSUAV_NlGg"
      },
      "source": [
        "#結論與發現"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn07j2Xo0t08"
      },
      "source": [
        "透過簡單的 LSTM 模型進行 NLG 任務 (自然語言生成)，可以發現成效仍有限，\n",
        "\n",
        "但已經可以發現自然語言模型已能大致上瞭解字與字之間的關聯性，\n",
        "\n",
        "若讀者有興趣深入理解其開發邏輯與原理，歡迎參考[《零基礎成為 AI 解夢大師秘笈》](https://aifreeblog.herokuapp.com/postlist/AI_DreamMaster/)，\n",
        "\n",
        "文章將從零引導讀者認識 Python 語法、爬蟲、Python Web 開發、AI基礎理論、NLP基礎概念，\n",
        "\n",
        "到最後引導讀者透過 Django 開發出使用者操作聊天介面！\n",
        "\n",
        "本系列教學從 AI 基礎開始，到電腦視覺以及自然語言領域，\n",
        "\n",
        "快速概要介紹了各式主流的 AI 核心技術，希望你/妳對於深度學習已經有基礎的認識！"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 更深入的教學與專案實作\n",
        "\n",
        "如果你/妳對於深入開發 AI 技術的專案有興趣，想嘗試更多、更有趣、更扎實的實務AI技術專案，\n",
        "\n",
        "歡迎參考我們從0到1的AI技術課程：[《學習 AI 一把抓：點亮人工智慧技能樹》](https://hahow.in/cr/slashie-ai-free-team)！"
      ],
      "metadata": {
        "id": "6X4jsuSKEN7E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9KkN2cXhgCB"
      },
      "source": [
        "# 課程文件\n",
        "\n",
        "### AI Foundations 課程清單\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 00 Preface 課程前言</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 01 MP model</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 02 Logistic Regression</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 03 Neural Network</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 04 Deep Neural Network</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 05 Convolution Neural Network</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 06 Handwriting Traditional Chinese Characters Recognition</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 07 Sentiment Analysis</a>\n",
        "- <a href=\"https://colab.research.google.com/github/AI-FREE-Team/Python-Basics/blob/master/documents/Lesson00%20Preface.ipynb\">Lesson 08 AI Dream Interpretation</a> (We are here now! --本篇課程--)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z9qd2bUFDlT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}